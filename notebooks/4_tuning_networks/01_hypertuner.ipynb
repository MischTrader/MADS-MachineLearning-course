{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rX6xVeZTZ9d"
      },
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/raoulg/MADS-MachineLearning-course/blob/master/notebooks/4_tuning_networks/01_hypertuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "</td>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru"
      ],
      "metadata": {
        "id": "7X45Nb_DTzrM",
        "outputId": "41a97e8d-0649-48a6-d6fc-88fa878fe497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray"
      ],
      "metadata": {
        "id": "bBQ2-4MfT_pl",
        "outputId": "7ba5cf36-ae3a-4708-822c-33ae3babf7ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray) (3.19.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray) (2.32.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.27.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.15.0)\n",
            "Downloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl (70.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.49.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visualize\n"
      ],
      "metadata": {
        "id": "dxcAh25VUZQK",
        "outputId": "b940ce34-ccb5-4637-bb1c-958d4c94d11c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting visualize\n",
            "  Downloading visualize-0.5.1.tar.gz (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from visualize) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from visualize) (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from visualize) (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from visualize) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->visualize) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->visualize) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->visualize) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->visualize) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->visualize) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->visualize) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->visualize) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->visualize) (1.17.0)\n",
            "Building wheels for collected packages: visualize\n",
            "  Building wheel for visualize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visualize: filename=visualize-0.5.1-py3-none-any.whl size=11479 sha256=412793e78b7f6ebc4b8833e48ad55aed3bb26637293981660de3877f37ec7e49\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/14/65/7e99ea4a7fa0f9a30cec02f837aed72f5f96ae9bb3586ee29e\n",
            "Successfully built visualize\n",
            "Installing collected packages: visualize\n",
            "Successfully installed visualize-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X7orqU67TZ9f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from plotly import graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from loguru import logger\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('dark_background')\n",
        "# import plotly.io as pio\n",
        "# pio.renderers.default = 'plotly_mimetype+notebook'\n",
        "\n",
        "import visualize\n",
        "DELETE = True # to delete the tunedir at the end of the notebook\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3c-IhL7TZ9i"
      },
      "source": [
        "This is a general reference notebook to explore the use of ray tuner.\n",
        "First, we define some global variables. This makes it easier to change the parameters for the full notebook at once and run everything.\n",
        "\n",
        "Set the max_epochs to something like 10 (this will take longer, but you will get better results), and experiments to 18. This way, you will have a good way to compare gridsearch with the other tuners because gridsearch will do 3 * 6 = 18 experiments.\n",
        "\n",
        "The reason I am setting this low is because i automated the testing of the notebook and it takes a long time to run all the experiments by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b-sSbO8TZ9i"
      },
      "outputs": [],
      "source": [
        "MAX_EPOCHS = 1\n",
        "N_EXPERIMENTS = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlKC62KZTZ9j"
      },
      "source": [
        "# Train function and config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcsprzN6TZ9k"
      },
      "source": [
        "Let's define some dicts to log the results of the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-griNG6TZ9k"
      },
      "outputs": [],
      "source": [
        "timer = {}\n",
        "best_config = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEisgDPwTZ9l"
      },
      "source": [
        "We will need a training function. This is also implemented in the mltrainer, but I put it here to show you how the details work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nge3AqZ4TZ9l"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, trainstreamer, lossfn, optimizer, steps):\n",
        "    model.train()\n",
        "    train_loss: float = 0.0\n",
        "    for _ in range(steps):\n",
        "        x, y = next(trainstreamer)\n",
        "        optimizer.zero_grad()\n",
        "        yhat = model(x)\n",
        "        loss = lossfn(yhat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQFhFgGpTZ9m"
      },
      "source": [
        "We validate on the validation set and return the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttso4UpaTZ9m"
      },
      "outputs": [],
      "source": [
        "def validate(model, validstreamer, lossfn, metric, steps):\n",
        "    model.eval()\n",
        "    valid_loss: float = 0.0\n",
        "    acc: float = 0.0\n",
        "    with torch.no_grad():\n",
        "        for _ in range(steps):\n",
        "            x, y = next(validstreamer)\n",
        "            yhat = model(x)\n",
        "            loss = lossfn(yhat, y)\n",
        "            valid_loss += loss.item()\n",
        "            acc += metric(y, yhat)\n",
        "    acc /= steps\n",
        "    return valid_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2VCAUVETZ9m"
      },
      "source": [
        "Getting the data requires a bit extra care. Because we will run experiments in parallel on all the cpu's available, we well use FileLock to make sure that loading the data does not conflict.\n",
        "\n",
        "Note we import functions inside the function we will later on pass to ray. This is because ray will serialize the function and send it to the workers. If we import the functions outside the function, the workers will not have access to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFp6wY1zTZ9m"
      },
      "outputs": [],
      "source": [
        "def get_data(tune_dir):\n",
        "    from filelock import FileLock\n",
        "    from mads_datasets import DatasetFactoryProvider, DatasetType\n",
        "    from mltrainer.preprocessors import PaddedPreprocessor\n",
        "    from mads_datasets import DatasetFactoryProvider, DatasetType\n",
        "    with FileLock(tune_dir / \".lock\"):\n",
        "        # we lock the datadir to avoid parallel instances trying to\n",
        "        # access the datadir\n",
        "        preprocessor = PaddedPreprocessor()\n",
        "        gesturesdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.GESTURES)\n",
        "        streamers = gesturesdatasetfactory.create_datastreamer(\n",
        "            batchsize=32, preprocessor=preprocessor\n",
        "        )\n",
        "        train = streamers[\"train\"]\n",
        "        valid = streamers[\"valid\"]\n",
        "    return train, valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPn08sUxTZ9n"
      },
      "source": [
        "We will use the same GRU model we used last lesson. You might improve this in a few ways (eg consider adding skip layers, conv1d, etc) but for clarity, lets keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajRU5nxJTZ9n"
      },
      "outputs": [],
      "source": [
        "class GRUmodel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: dict,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=config[\"input_size\"],\n",
        "            hidden_size=int(config[\"hidden_size\"]),\n",
        "            dropout=config[\"dropout\"],\n",
        "            batch_first=True,\n",
        "            num_layers=int(config[\"num_layers\"]),\n",
        "        )\n",
        "        self.linear = nn.Linear(int(config[\"hidden_size\"]), config[\"output_size\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        last_step = x[:, -1, :]\n",
        "        yhat = self.linear(last_step)\n",
        "        return yhat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ6Y4LgdTZ9n"
      },
      "source": [
        "Now, we have all the ingredients we need to run the tuner.\n",
        "We create a function that:\n",
        "- loads the data with a lock\n",
        "- creates the model\n",
        "- trains the model\n",
        "- validates the model\n",
        "- reports the results to ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh-SuEPwTZ9o"
      },
      "outputs": [],
      "source": [
        "def tune_model(config: dict):\n",
        "    from mltrainer.metrics import Accuracy\n",
        "\n",
        "    # load data\n",
        "    train, valid = get_data(config[\"tune_dir\"])\n",
        "    trainsteps = len(train)\n",
        "    validsteps = len(valid)\n",
        "    trainstreamer = train.stream()\n",
        "    validstreamer = valid.stream()\n",
        "\n",
        "    # create model with config\n",
        "    model = GRUmodel(config)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    metric = Accuracy()\n",
        "\n",
        "    for _ in range(config[\"epochs\"]):\n",
        "        # train and validate\n",
        "        train_loss = train_fn(model, trainstreamer, loss_fn, optimizer, trainsteps)\n",
        "        valid_loss, accuracy = validate(model, validstreamer, loss_fn, metric, validsteps)\n",
        "\n",
        "        # report to ray\n",
        "        ray.train.report({\n",
        "            \"valid_loss\": valid_loss / validsteps,\n",
        "            \"train_loss\": train_loss / trainsteps,\n",
        "            \"accuracy\" : accuracy,\n",
        "            })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xZDMfgqTZ9o"
      },
      "source": [
        "Let's try this, to see if everything works as expected.\n",
        "Note that it typically does not make sense to use accelaration here; we will typically have 1 GPU, but 10 or 16 CPUS. We gain much more from parallelizing the experiments than we gain by running the model faster on a single GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR9roq5rTZ9o"
      },
      "outputs": [],
      "source": [
        "tune_dir = Path(\"logs/test/\").resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyPTi-s1TZ9o"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"input_size\": 3,\n",
        "    \"output_size\": 20,\n",
        "    \"dropout\": 0.1,\n",
        "    \"hidden_size\": 64,\n",
        "    \"num_layers\": 2,\n",
        "    \"epochs\": MAX_EPOCHS,\n",
        "    \"tune_dir\": tune_dir,\n",
        "}\n",
        "tune_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tAbyicbTZ9o"
      },
      "source": [
        "# Random search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ME0zNfTZ9p"
      },
      "source": [
        "Now, let's do a random search.\n",
        "\n",
        "First, we define the search space. We can specify specific values, but also distributions.\n",
        "For the hidden size, we will use randint, which will sample from a uniform distribution of integers.\n",
        "The same for the number of layers.\n",
        "\n",
        "The `tune.run` function runs the hypertuning. It will sample from the search space, and create a specific config for each experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xCORkZATZ9p"
      },
      "outputs": [],
      "source": [
        "\n",
        "config = {\n",
        "    \"input_size\": 3,\n",
        "    \"output_size\": 20,\n",
        "    \"dropout\": 0.05,\n",
        "    \"epochs\": MAX_EPOCHS,\n",
        "    \"hidden_size\": tune.randint(16, 512),\n",
        "    \"num_layers\": tune.randint(1, 8),\n",
        "    \"tune_dir\": tune_dir,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL2Pd0N-TZ9p"
      },
      "source": [
        "Let's test this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXTYDwOBTZ9p"
      },
      "outputs": [],
      "source": [
        "tic = time.time()\n",
        "analysis = tune.run(\n",
        "    tune_model,\n",
        "    config=config,\n",
        "    metric=\"valid_loss\",\n",
        "    mode=\"min\",\n",
        "    storage_path=str(tune_dir),\n",
        "    num_samples=N_EXPERIMENTS,\n",
        "    stop={\"training_iteration\": MAX_EPOCHS},\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "timer[\"ray_random\"] = time.time() - tic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUT53Vf2TZ9p"
      },
      "outputs": [],
      "source": [
        "def plot_contour(df, x, y, z, start=0.90, end=1.0, size=0.01):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Contour(\n",
        "            z=df[z],\n",
        "            x=df[x],\n",
        "            y=df[y],\n",
        "            contours=dict(\n",
        "                coloring='heatmap',\n",
        "                showlabels=True,  # show labels on contours\n",
        "                start=start,       # start of the contour range\n",
        "                end=end,          # end of the contour range\n",
        "                size=size,\n",
        "            ),\n",
        "            colorscale=\"plasma\",\n",
        "            colorbar=dict(\n",
        "                title='Accuracy'\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df[x],\n",
        "            y=df[y],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                color='black',\n",
        "                size=8,\n",
        "                symbol='circle'\n",
        "            ),\n",
        "            customdata=df['accuracy'],  # Pass accuracy values for hover text\n",
        "            hovertemplate=(\n",
        "                'Hidden Size: %{x}<br>'\n",
        "                'Number of Layers: %{y}<br>'\n",
        "                'Accuracy: %{customdata:.4f}<extra></extra>'\n",
        "            ),\n",
        "            name='Data Points'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Contour Plot\",\n",
        "        xaxis_title=\"Hidden Size\",\n",
        "        yaxis_title=\"Number of Layers\",\n",
        "        xaxis=dict(showgrid=False),  # Remove x-axis grid lines\n",
        "        yaxis=dict(showgrid=False),\n",
        "        plot_bgcolor='white',        # Set background color to white\n",
        "        paper_bgcolor='white'\n",
        "    )\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv-7aRXyTZ9p"
      },
      "outputs": [],
      "source": [
        "all_results = analysis.results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btnfx5MNTZ9p"
      },
      "outputs": [],
      "source": [
        "random = analysis.results_df\n",
        "plot_contour(random, \"config/hidden_size\", \"config/num_layers\", \"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQvKvkAoTZ9q"
      },
      "source": [
        "As you can see, the search space is sort of randomly sampled.\n",
        "Even though big parts are unexplored, it still looks like we find some hotspots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msm0VFOpTZ9q"
      },
      "outputs": [],
      "source": [
        "best = analysis.get_best_config()\n",
        "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
        "best_config[\"random\"] = best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di-qM6ojTZ9q"
      },
      "source": [
        "So, we searched the hyperparameter space. Problem is, these spaces potentially can get\n",
        "pretty big. Let's imagine you have 10 hyperparameters, and every hyperparameter has 5\n",
        "possible (relevant) values, you already have $5^{10}$ possible combinations, which is almost 10 million. Even if checking of every configuration would take just 1 second, it would take more than a 100 days to check them all...This\n",
        "space can grow out of control pretty fast.\n",
        "\n",
        "Lets look at the best results we found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27_TO61OTZ9q"
      },
      "outputs": [],
      "source": [
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2GOUIWxTZ9q"
      },
      "outputs": [],
      "source": [
        "columns = [\"config/hidden_size\", \"config/num_layers\", \"accuracy\"]\n",
        "visualize.parallel_plot(analysis, columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBZ03vZjTZ9q"
      },
      "source": [
        "Note how the mean scores are sort of randomly distributed. This is a direct\n",
        "effect of random guessing parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-na-DxzaTZ9q"
      },
      "source": [
        "# Grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4tizTggTZ9r"
      },
      "source": [
        "We can do this more rigorous with a gridsearch.\n",
        "The upside of this technique is that you will test every configuration.\n",
        "\n",
        "A huge downside is the inefficiency. Also, you will run experiments with combinations that might be not very promising, but very slow, which is pretty inefficient.\n",
        "\n",
        "\n",
        "One way to handle this is to use doubling as a strategy to scan the range: 16, 32, ..., 512. This is a bit more efficient.\n",
        "\n",
        "Typically, this can be a good idea for a first scan, to get a rough idea of the space.\n",
        "After you have done this, you can narrow your searchspace, and do a more fine grained search zoomed in on areas that seem promising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8urX_RiTZ9r"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"input_size\": 3,\n",
        "    \"output_size\": 20,\n",
        "    \"dropout\": 0.1,\n",
        "    \"epochs\": MAX_EPOCHS,\n",
        "    \"hidden_size\": tune.grid_search([16, 32, 64, 128, 256, 512]),\n",
        "    \"num_layers\": tune.grid_search([2, 4, 8]),\n",
        "    \"tune_dir\": tune_dir,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye5HBymJTZ9r"
      },
      "outputs": [],
      "source": [
        "\n",
        "tic = time.time()\n",
        "\n",
        "analysis = tune.run(\n",
        "    tune_model,\n",
        "    config=config,\n",
        "    metric=\"valid_loss\",\n",
        "    mode=\"min\",\n",
        "    storage_path=str(tune_dir),\n",
        "    stop={\"training_iteration\": MAX_EPOCHS},\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "timer[\"ray_grid\"] = time.time() - tic\n",
        "\n",
        "best = analysis.get_best_config()\n",
        "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
        "best_config[\"grid\"] = best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTCapk1gTZ9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "all_results = pd.concat([all_results, analysis.results_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCafrKalTZ93"
      },
      "outputs": [],
      "source": [
        "grid = analysis.results_df\n",
        "plot_contour(grid, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc3MltSzTZ93"
      },
      "source": [
        "As you can see, we get a systematic scan, but large parts of the space are still unexplored, and we also explored parts that are really not very promising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0RgZftJTZ93"
      },
      "outputs": [],
      "source": [
        "visualize.parallel_plot(analysis, columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuPS_Q6wTZ93"
      },
      "outputs": [],
      "source": [
        "visualize.plot_timers(timer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krltOMtCTZ93"
      },
      "source": [
        "# Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAY8SXLTTZ94"
      },
      "source": [
        "Now, we improve the search algorithm with a bayesian optimization.\n",
        "\n",
        "Note that the bayesian search algorithm will only work with continuous parameters. This is a problem for the number of layers, which is a discrete parameter. I fixed this by simply casting the parameters to an integer inside the model, which is not very elegant but it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1St3_iGxTZ94"
      },
      "outputs": [],
      "source": [
        "from ray.tune.search.bayesopt import BayesOptSearch\n",
        "\n",
        "bayesopt = BayesOptSearch(metric=\"valid_loss\", mode=\"min\")\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"input_size\": 3,\n",
        "    \"output_size\": 20,\n",
        "    \"dropout\": 0.1,\n",
        "    \"epochs\": MAX_EPOCHS,\n",
        "    \"hidden_size\": tune.uniform(16, 512),\n",
        "    \"num_layers\": tune.uniform(1, 8),\n",
        "    \"tune_dir\": tune_dir,\n",
        "}\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "analysis = tune.run(\n",
        "    tune_model,\n",
        "    config=config,\n",
        "    metric=\"valid_loss\",\n",
        "    mode=\"min\",\n",
        "    storage_path=str(tune_dir),\n",
        "    num_samples=N_EXPERIMENTS,\n",
        "    stop={\"training_iteration\": MAX_EPOCHS},\n",
        "    search_alg=bayesopt,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "timer[\"ray_bayes\"] = time.time() - tic\n",
        "\n",
        "best = analysis.get_best_config()\n",
        "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
        "best_config[\"bayes\"] = best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eqxm9v4-TZ94"
      },
      "outputs": [],
      "source": [
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJnRFxmtTZ94"
      },
      "outputs": [],
      "source": [
        "all_results = pd.concat([all_results, analysis.results_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GOtiCckTZ94"
      },
      "outputs": [],
      "source": [
        "bayes = analysis.results_df\n",
        "plot_contour(bayes, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJ_A-nyTZ94"
      },
      "source": [
        "As you can see, bayes really focuses on the promising areas. It is much more efficient than random search, and also more efficient than grid search.\n",
        "We have set the `random_search_steps` to 5, this means that we will do 5 random searches first, to get a good initial idea of the space. As you can see, 5 is a bit low, because it might lead to premature converging to a local optimum. You can increase this number to get a better initial idea of the space, but you will also need to increase the number of iterations after the initial random scan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXY8ZQehTZ94"
      },
      "outputs": [],
      "source": [
        "visualize.parallel_plot(analysis, columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ1svNg6TZ95"
      },
      "outputs": [],
      "source": [
        "visualize.plot_timers(timer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYz-fU6hTZ95"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict(best_config, orient=\"index\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKCgoQ-kTZ95"
      },
      "source": [
        "# Hyperband\n",
        "\n",
        "Hyperband aborts runs early. Configs that are unpromising are abandoned before they complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYGQcPIoTZ95"
      },
      "outputs": [],
      "source": [
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "\n",
        "scheduler = AsyncHyperBandScheduler(\n",
        "    time_attr=\"training_iteration\", grace_period=1, reduction_factor=3, max_t=MAX_EPOCHS\n",
        ")\n",
        "\n",
        "config = {\n",
        "    \"input_size\": 3,\n",
        "    \"output_size\": 20,\n",
        "    \"dropout\": 0.1,\n",
        "    \"epochs\": MAX_EPOCHS,\n",
        "    \"hidden_size\": tune.randint(16, 512),\n",
        "    \"num_layers\": tune.randint(1, 8),\n",
        "    \"tune_dir\": tune_dir,\n",
        "}\n",
        "\n",
        "tic = time.time()\n",
        "analysis = tune.run(\n",
        "    tune_model,\n",
        "    config=config,\n",
        "    metric=\"valid_loss\",\n",
        "    mode=\"min\",\n",
        "    storage_path=str(tune_dir),\n",
        "    num_samples=N_EXPERIMENTS,\n",
        "    stop={\"training_iteration\": MAX_EPOCHS},\n",
        "    scheduler=scheduler,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "timer[\"ray_hyperband\"] = time.time() - tic\n",
        "\n",
        "best = analysis.get_best_config()\n",
        "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
        "best_config[\"hyperband\"] = best\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwtjBJQSTZ95"
      },
      "source": [
        "If you study the iter column, you will see that some runs have been stopped early. This is exactly the point of hyperband: it tries to allocate resources to the most promising configurations.\n",
        "\n",
        "The downside of this, is that if you try to get a good view of the space, you get distorted results because comparing a model that has trained just 1 or 3 epochs with a model that has trained 100 epochs is not very fair..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYO1cVB2TZ96"
      },
      "outputs": [],
      "source": [
        "all_results = pd.concat([all_results, analysis.results_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL1ZFWlrTZ96"
      },
      "outputs": [],
      "source": [
        "hyperband = analysis.results_df\n",
        "plot_contour(hyperband, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF22sBYYTZ96"
      },
      "outputs": [],
      "source": [
        "visualize.parallel_plot(analysis, columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAZ9i7VnTZ96"
      },
      "source": [
        "But note that it is faster! This means you could do more runs in the same time, which might be a good tradeoff!\n",
        "Typically you will get better results if you scan the searchspace better by doing more runs, aborting the ones that are not promising and going on with the ones that seem to yield good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxyn0X0UTZ96"
      },
      "outputs": [],
      "source": [
        "visualize.plot_timers(timer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1WrQKJdTZ96"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame.from_dict(best_config, orient=\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaCr5iZOTZ96"
      },
      "source": [
        "# Hyperbayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivxLV86sTZ96"
      },
      "source": [
        "It is possible to combine Hyperband with Bayesian optimization, implemented as `TuneBOHB` in ray.\n",
        "However, `TuneBOHB` in ray is dependent on `hpbandster`, which is not maintained anymore.\n",
        "Unfortunately, a dependency of `hpbandster` is `netifaces`, which is also not maintained anymore.\n",
        "While `netifaces` does work on some hardware, it fails to build on some other hardware, which is a problem.\n",
        "\n",
        "To still use BOHB type algoritms, I am planning to implement either SMAC3 or BEHB in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5yif03sTZ97"
      },
      "source": [
        "# HyperOpt\n",
        "HyperOpt provides gradient/derivative-free optimization. They implemented TPE, which stands for Tree-structured Parzen Estimator. This is a bayesian optimization algorithm that uses a tree to model the distribution of the objective function. The main advantage is that TPE also works with discrete parameters and conditional search spaces which is a problem for the bayesian optimization.\n",
        "\n",
        "For example, you could do things like this:\n",
        "```python\n",
        "conditional_space = {\n",
        "    \"activation\": hp.choice(\n",
        "        \"activation\",\n",
        "        [\n",
        "            {\"activation\": \"relu\", \"mult\": hp.uniform(\"mult\", 1, 2)},\n",
        "            {\"activation\": \"tanh\"},\n",
        "        ],\n",
        "    ),\n",
        "}\n",
        "```\n",
        "\n",
        "Where you have a discrete parameter `activation` that has a conditional parameter `mult`. This means that if you choose `relu`, you also need to set `mult`, but if you choose `tanh`, you do not need to set `mult`. See [documentation of ray](https://docs.ray.io/en/latest/tune/examples/hyperopt_example.html) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsg8Xo2cTZ97"
      },
      "outputs": [],
      "source": [
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "search = HyperOptSearch()\n",
        "\n",
        "scheduler = AsyncHyperBandScheduler(\n",
        "    time_attr=\"training_iteration\", grace_period=1, reduction_factor=3, max_t=MAX_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1v6KaM2TZ97"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"input_size\": 3,\n",
        "    \"output_size\": 20,\n",
        "    \"dropout\": 0.1,\n",
        "    \"epochs\": MAX_EPOCHS,\n",
        "    \"hidden_size\": tune.randint(16, 512),\n",
        "    \"num_layers\": tune.randint(1, 8),\n",
        "    \"tune_dir\": tune_dir,\n",
        "}\n",
        "\n",
        "tic = time.time()\n",
        "analysis = tune.run(\n",
        "    tune_model,\n",
        "    config=config,\n",
        "    metric=\"valid_loss\",\n",
        "    mode=\"min\",\n",
        "    storage_path=str(tune_dir),\n",
        "    num_samples=N_EXPERIMENTS,\n",
        "    stop={\"training_iteration\": MAX_EPOCHS},\n",
        "    search_alg=search,\n",
        "    scheduler=scheduler,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "timer[\"ray_hyperopt\"] = time.time() - tic\n",
        "\n",
        "best = analysis.get_best_config()\n",
        "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
        "best_config[\"hyperopt\"] = best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "equWeBdXTZ97"
      },
      "outputs": [],
      "source": [
        "all_results = pd.concat([all_results, analysis.results_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuMOgbh5TZ97"
      },
      "outputs": [],
      "source": [
        "hyperbayes = analysis.results_df\n",
        "plot_contour(hyperbayes, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.3, size=0.05)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgFlijNVTZ97"
      },
      "source": [
        "This model combines bayesian optimization with hyperband. This is a good idea, because it combines the efficiency of bayesian optimization with the speed of hyperband."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYc9taAdTZ98"
      },
      "outputs": [],
      "source": [
        "visualize.parallel_plot(analysis, columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9mQ6BKVTZ98"
      },
      "outputs": [],
      "source": [
        "visualize.plot_timers(timer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCHGnsLNTZ98"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame.from_dict(best_config, orient=\"index\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73YOIfGjTZ98"
      },
      "outputs": [],
      "source": [
        "contour = all_results[all_results[\"training_iteration\"] == MAX_EPOCHS]\n",
        "plot_contour(contour, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "564GKaomTZ98"
      },
      "outputs": [],
      "source": [
        "stop = time.time() - start\n",
        "print(stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZbo59PLTZ98"
      },
      "outputs": [],
      "source": [
        "if DELETE:\n",
        "    import shutil\n",
        "    shutil.rmtree(tune_dir)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}